<!doctype html>
<html lang="en-US">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0" />
        <meta name="description" content="FRIEREN, a novel no-reference image quality assessment, effectively and accurately evaluates the face detail quality scaled by different interpolations with low computational complexity." />
        <meta property="og:title" content="FRIEREN: A Lightweight System for Face Resizing Image Detail Quality Evaluation via Robust Estimation of Image Naturalness" />
        <meta property="og:description" content="FRIEREN, a novel no-reference image quality assessment, effectively and accurately evaluates the face detail quality scaled by different interpolations with low computational complexity." />
        <meta property="og:url" content="https://NTUneillee.github.io/research/friereniqa/" />
        
        <meta property="og:image" content="https://NTUneillee.github.io/images/post/FRIERENlogo.webp" />
        
        <meta property="og:image:width" content="1200" />
        <meta property="og:image:height" content="630" />

        <meta name="twitter:title" content="FRIEREN: A Lightweight System for Face Resizing Image Detail Quality Evaluation via Robust Estimation of Image Naturalness" />
        <meta name="twitter:description" content="FRIEREN, a novel no-reference image quality assessment, effectively and accurately evaluates the face detail quality scaled by different interpolations with low computational complexity." />
        
        <meta name="twitter:image" content="https://NTUneillee.github.io/images/post/FRIERENlogo.webp" />
        
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="keywords" content="Camera Image Quality, Image Processing, Color Science" />
        <title>FRIEREN: A Lightweight System for Face Resizing Image Detail Quality Evaluation via Robust Estimation of Image Naturalness</title>

        <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

        <link rel="stylesheet" href="https://NTUneillee.github.io/academic-assets/css/spf-bulma.min.css" />
        <link rel="stylesheet" href="https://NTUneillee.github.io/academic-assets/css/fontawesome.all.min.css" />
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.5/css/academicons.min.css" integrity="sha256-SzrCOBJbGVFMahewkgjwnApaV2+av1DwMAA+/QGLyZw=" crossorigin="anonymous" />
        <link rel="stylesheet" href="https://NTUneillee.github.io/academic-assets/css/spf-index.css" />

        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
        <script defer src="https://NTUneillee.github.io/academic-assets/js/fontawesome.all.min.js"></script>

        <script>
            window.MathJax = {
                tex: {
                    inlineMath: [["$", "$"]],
                },
                chtml: {
                    scale: 1.1,
                    mtextInheritFont: true,
                },
                svg: {
                    fontCache: 'global'
                }
            };
        </script>
        <script id="MathJax-script" defer src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.js" integrity="sha256-qoRlVrS5NAnXSSSiMfFXwK8C9obG11Iybe4h2+bQYR4=" crossorigin="anonymous"></script>

        <style>
             
            mjx-container {
                -webkit-font-smoothing: antialiased;
                -moz-osx-font-smoothing: grayscale;
            }

            mjx-container svg {
                shape-rendering: geometricPrecision;
            }

             
            .method-image-container,
            .results-image-container {
                display: inline-block;
                border: 1px solid #e0e0e0;
                border-radius: 12px;
                padding: 1rem;
                background-color: #f5f5f5;
                box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
                margin: 0.75rem 0;
            }

             
            .hero.is-light .method-image-container,
            .hero.is-light .results-image-container {
                border: 1px solid #e0e0e0;
                background-color: #ffffff;
                box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
            }

            .method-image-container img,
            .results-image-container img {
                border-radius: 8px;
                max-width: 100%;
                height: auto;
                display: block;
            }

            .publication-title {
                line-height: 1.5 !important;
            }

             
            .publication-authors .author-block sup {
                margin-left: 0.1em !important;
                margin-right: 0 !important;
                padding-left: 0 !important;
            }

             
            .publication-authors .author-block a + sup,
            .publication-authors .author-block span + sup {
                margin-left: 0.1em !important;
            }

             
            .container.is-max-desktop {
                max-width: 1080px;
            }

            .columns.is-centered {
                justify-content: center;
            }

             
            img {
                max-width: 100% !important;
                height: auto !important;
                width: 100% !important;
            }

             
            .hero-body {
                padding-left: 0;
                padding-right: 0;
            }

             
            .hero.teaser {
                padding-top: 0;
                margin-top: 0;
            }

            .hero.teaser .hero-body {
                padding-top: 0;
            }

            .card-container {
                padding-left: 0 !important;
                padding-right: 0 !important;
            }

             
            .title.is-1 {
                font-size: 2.5rem;
            }

             
            .publication-authors:has(.eql-cntrb) {
                margin-bottom: 0.5rem !important;
            }

            .column.has-text-centered {
                margin-top: 0;
                padding-top: 0.5rem;
            }

             
            section#BibTeX pre {
                overflow-x: auto;
                white-space: pre;
                word-wrap: normal;
                width: 100%;
                display: block;
                box-sizing: border-box;
                margin: 0;
                padding: 1.25rem;
                border: 1px solid #e0e0e0;
                border-radius: 4px;
            }

             
            section.hero.is-light#BibTeX pre {
                background-color: #ffffff;
            }

             
            section#BibTeX:not(.is-light) pre {
                background-color: #f5f5f5;
            }

            #BibTeX code {
                white-space: pre;
                word-wrap: normal;
                display: block;
            }

            #BibTeX .column {
                width: 100%;
            }

             
            .content mjx-container[jax="CHTML"][display="false"] {
                display: inline !important;
                vertical-align: baseline !important;
                margin: 0 !important;
            }

            .content mjx-container {
                line-height: 0 !important;
            }

             
            .content mjx-container[jax="CHTML"][display="true"] {
                overflow-x: auto;
                overflow-y: hidden;
                max-width: 100%;
                display: block !important;
            }

             
            .table-wrapper {
                width: 100%;
                overflow-x: auto;
                -webkit-overflow-scrolling: touch;
                margin: 1rem 0;
            }

             
            .content table {
                border-collapse: collapse;
                width: 100%;
                display: table;
                border-top: 2px solid #666666;
                border-bottom: 2px solid #666666;
            }

            .content table th,
            .content table td {
                white-space: nowrap;
            }

             
            @media screen and (min-width: 769px) {
                .hero.teaser .subtitle.content {
                    font-size: 1rem !important;
                    line-height: 1.7 !important;
                }

                .content,
                .content p,
                .content li {
                    font-size: 1rem !important;
                    line-height: 1.7 !important;
                }
            }

             
            @media screen and (max-width: 768px) {
                 
                html {
                    font-size: 16px !important;
                }

                body {
                    font-size: 1rem !important;
                    line-height: 1.6 !important;
                }


                .hero-body {
                    padding: 2rem 0.75rem !important;
                }

                .title.is-1 {
                    font-size: 1.75rem !important;
                    line-height: 1.3 !important;
                    margin-bottom: 1rem !important;
                }

                .title.is-3 {
                    font-size: 1.5rem !important;
                    line-height: 1.3 !important;
                    margin-bottom: 1rem !important;
                }

                .title.is-5 {
                    font-size: 1.125rem !important;
                    line-height: 1.4 !important;
                }

                .is-size-4 {
                    font-size: 1.25rem !important;
                    line-height: 1.4 !important;
                }

                .is-size-5 {
                    font-size: 1.0625rem !important;
                    line-height: 1.5 !important;
                }

                .subtitle {
                    font-size: 1rem !important;
                    line-height: 1.6 !important;
                }

                .content,
                .content p,
                .content li {
                    font-size: 1rem !important;
                    line-height: 1.8 !important;
                }

                .content h3 {
                    font-size: 1.25rem !important;
                    margin-top: 1.5rem !important;
                    margin-bottom: 0.75rem !important;
                }

                .content h4 {
                    font-size: 1.125rem !important;
                    margin-top: 1.25rem !important;
                    margin-bottom: 0.75rem !important;
                }

                .container.is-max-desktop {
                    padding-left: 0.75rem !important;
                    padding-right: 0.75rem !important;
                    max-width: 100% !important;
                }

                .section {
                    padding: 2.5rem 0.75rem !important;
                }

                .columns {
                    margin: 0 !important;
                }

                .column {
                    padding: 0 0.75rem !important;
                }

                .column.is-half {
                    width: 100% !important;
                }

                .column.is-four-fifths {
                    width: 100% !important;
                }

                .publication-links .link-block {
                    display: inline-block;
                    margin-bottom: 0;
                    margin-right: 0.5rem;
                }

                .publication-links .button {
                    width: auto;
                    font-size: 1.0625rem !important;
                    padding: 0.5rem 0.75rem !important;
                }

                iframe {
                    width: 100% !important;
                    height: auto !important;
                    aspect-ratio: 16 / 9;
                }

                pre {
                    font-size: 0.875rem;
                    overflow-x: auto;
                    padding: 1rem !important;
                    margin: 0 !important;
                    border-radius: 4px !important;
                    width: 100% !important;
                    box-sizing: border-box !important;
                }

                pre code {
                    font-size: 0.875rem !important;
                }

                #BibTeX .column {
                    padding: 0 0.75rem !important;
                    width: 100% !important;
                }

                #BibTeX pre {
                    margin: 0 !important;
                    width: 100% !important;
                }

                .card-container {
                    padding: 1.5rem !important;
                    margin-bottom: 2rem !important;
                    margin-left: 0 !important;
                    margin-right: 0 !important;
                }

                .card-container h4 {
                    font-size: 1.5rem !important;
                }

                .card-container p,
                .card-container li {
                    font-size: 1.25rem !important;
                }

                 
                .has-text-justified {
                    text-align: left !important;
                }

                 
                .hero,
                .section {
                    overflow-x: hidden !important;
                }

                img {
                    max-width: 100% !important;
                    height: auto !important;
                }
            }
        </style>
    </head>

    <body>
        <section class="hero">
            <div class="hero-body">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered">
                            <h1 class="title is-1 publication-title">
                                FRIEREN: A Lightweight System for Face Resizing Image Detail Quality Evaluation via Robust Estimation of Image Naturalness
                            </h1>
                            
                            <div class="is-size-4 publication-authors" style="margin-bottom: 0.5rem;">
                                <span class="conference-block">IEEE APCCAS 2025</span>
                            </div>
                            

                            <div class="is-size-5 publication-authors" style="margin-bottom: 0.5rem;">
                                
                                
                                <span class="author-block"><strong><a href="https://ntuneillee.github.io/" target="_blank">Yuan-Kang Lee</a></strong><sup>1</sup>,
                                </span>
                                
                                <span class="author-block"><a href="https://brianchen1120.github.io" target="_blank">Kuan-Lin Chen</a><sup>1</sup>,
                                </span>
                                
                                <span class="author-block"><a href="https://www.ee.ntu.edu.tw/profile1.php?teacher_id=942019" target="_blank">Jian-Jiun Ding</a><sup>1</sup>
                                </span>
                                
                                
                            </div>

                            <div class="is-size-5 publication-authors" style="margin-bottom: 0.5rem;">
                                
                                
                                <span class="author-block"><sup>1</sup>Graduate Institute of Communication Engineering, National Taiwan University</span>
                                
                                
                            </div>

                            <div class="column has-text-centered" style="margin-top: 0; padding-top: 0.5rem;">
                                <div class="publication-links">
                                    
                                    <span class="link-block">
                                        <a href="https://brianchen1120.github.io/files/FRIEREN%20A%20Lightweight%20System%20for%20Face%20Resizing%20Image%20Detail%20Quality%20Evaluation%20via%20Robust%20Estimation%20of%20Image%20Naturalness.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon"><i class="fas fa-file-pdf"></i></span>
                                            <span>Paper</span>
                                        </a>
                                    </span>
                                    

                                    

                                    
                                    <span class="link-block">
                                        <a href="https://ntuneillee.github.io/research/friereniqa/" target="_blank" class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon"><i class="fab fa-github"></i></span>
                                            <span>Code</span>
                                        </a>
                                    </span>
                                    

                                    

                                    
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="hero teaser">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column">
                        <div class="hero-body">
                            
                            <img src="https://NTUneillee.github.io/images/post/FRIERENlogo.webp" alt="FRIEREN: A Lightweight System for Face Resizing Image Detail Quality Evaluation via Robust Estimation of Image Naturalness" style="width: 100%; display: block;" />
                            
                            
                            <div class="subtitle has-text-centered content" style="width: 100%; margin-top: 1.5rem; line-height: 1.7;">
                                FRIEREN, a novel no-reference image quality assessment, effectively and accurately evaluates the face detail quality scaled by different interpolations with low computational complexity. From human perception, the ranking of detail qualities should be: Lanczos &gt; bicubic &gt; bilinear &gt; nearest-neighbor interpolation. It is recommended to zoom in on-screen to observe the different visual effects of different interpolations.
                            </div>
                            
                        </div>
                    </div>
                </div>
            </div>
        </section>

        
        <section class="section hero is-light">
            <div class="container is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            In real-time video conferencing systems, webcams often apply image resizing methods, such as nearest-neighbor, bilinear, bicubic, and Lanczos interpolation, to highlight facial regions and enhance user experience. However, interpolations introduce significant high-frequency artifacts that distort perceived image quality. Our work discovered that existing state-of-the-art image quality assessments (IQA) greatly overestimate the sharpness in images resized by nearest-neighbor interpolation, failing to extract useful information in the image’s high-frequency components. To address this, we propose FRIEREN (Face Resizing Image Detail Quality Evaluation via Robust Estimation of Image Naturalness), a novel IQA for detail quality evaluation that integrates measures of image naturalness, including motion noise, spatial noise, and HVS-based sharpness. Designed features are fed to Kolmogorov-Arnold Networks (KANs) for quality prediction. Experimental results show that FRIEREN effectively and accurately evaluates the face image detail quality scaled by different interpolations with low computational complexity,making it suitable for quality-aware vision systems.
                        </div>
                    </div>
                </div>
            </div>
        </section>
        

        
            <section class="section">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column">
                            <h2 class="title is-3 has-text-centered">Contributions</h2>
                            <div class="content has-text-justified">
                                <p>It can be observed that the
face image enlarged by the nearest-neighbor method is heavily
affected by severe block artifacts. Existing advanced IQAs
mistakenly interpret noise-related high-frequency components as textures, underscoring a critical limitation in their ability
to distinguish useful high-frequency information in images. To overcome this drawback, FRIEREN is based on two main observations of the human visual system:</p>
<ul>
<li>
<p>Edge-related high-frequency elements are the most important factors in how humans evaluate image sharpness.</p>
</li>
<li>
<p>For the HVS perception, the ranking of detail quality
levels in facial images after applying different interpolations is as follows: Lanczos, bicubic, bilinear, and nearest-neighbor method (from the best to the worst).</p>
</li>
</ul>
<p>To the best of our knowledge, our work is the first one that addresses interpolation effects on facial image quality. The detail quality degradation induced by interpolation can be quantified using our proposed motion and spatial noises and HVS-based sharpness calculation effectively.</p>

                            </div>
                        </div>
                    </div>
                </div>
            </section>
            

            
            <section class="section hero is-light">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column">
                            <h2 class="title is-3 has-text-centered">Method Overview</h2>
                            <div class="content has-text-justified">
                                <h3 id="quality-evaluation-using-kans">Quality Evaluation using KANs</h3>
<p>Predicting image quality requires a regression model that maps features to quality scores to enhance alignment with human visual judgment. We employ Kolmogorov-Arnold Networks (KANs) for the regression in FRIEREN. KANs,
based on the Kolmogorov-Arnold theorem, differ from MLPs by avoiding linear outputs. It enables KANs to effectively capture non-linear relationships, making them a way better regression model for simulating human perception. We use
the Adam (Adaptive moment estimation) optimizer with $\beta_1$ = 0.9 and $\beta_2$ = 0.999 to update the model parameters for optimal quality prediction performance. To train the quality prediction model, 64% of the images were randomly selected for training, with 16% used for validation and the remaining 20% for testing. The proposed image features: motion noise, spatial noise, and HVS-based sharpness measure in FRIEREN are introduced in the following sections.</p>
<h3 id="feature-motion-noise-estimation">Feature: Motion Noise Estimation</h3>
<p>High-level temporal noise disrupts visual understanding of
multimedia content. It cannot be completely eliminated due
to the sensor limitations and environmental conditions. Hence,
determining the effect of temporal noise is essential to quantify
the facial details that a camera can reproduce in images. To
adapt FRIEREN into a no-reference method, motion noise
is introduced to calculate the temporal noise influence using
only one single image frame. Frame modification is applied
for motion noise estimation. Denoted $I$ as the original image
frame. By discarding the last row and column of pixels from
$I$, the new image frame $I’$ with slightly different content is
created. Then, image frames $I$ and $I’$ are both enlarged to the
same dimensions, simulating motion variations typically seen
in a video sequence. Let $D$ be denoted as the absolute pixel-wise difference between the two frames. The motion noise of the image frame, $\sigma_m$, is calculated as:</p>
<p>$$ \sigma_m = \sqrt{ \frac{1}{M \cdot N} \sum_{i=1}^{M} \sum_{j=1}^{N} (D_{ij} - \mu_D)^2 } $$</p>
<p>where $\mu_D$ represents the average of the frame difference and $M$ and $N$ are the height and width of $I$, respectively.</p>
<h3 id="feature-spatial-noise-estimation">Feature: Spatial Noise Estimation</h3>

  <div class="has-text-centered" style="margin: 1.0rem 0;">
      <div class="method-image-container">
          <img src="https://NTUneillee.github.io/academic-assets/images/ProposedNoiseEst.webp" alt="**Spatial noise estimation framework in FRIEREN**: By excluding the edge-related components in high frequency layers, the DWT-based spatial noise estimation enables FRIEREN to quantify the level of noise in an image effectively and accurately." />
          
          <p style="margin-top: 1rem; font-size: 0.95rem; color: #666; line-height: 1.6;"><strong>Spatial noise estimation framework in FRIEREN</strong>: By excluding the edge-related components in high frequency layers, the DWT-based spatial noise estimation enables FRIEREN to quantify the level of noise in an image effectively and accurately.</p>
          
      </div>
  </div>

<p>The 2D Discrete Wavelet Transform (DWT) decomposes an image into LL, LH, HL, and HH sub-bands, capturing varying detail levels. High-frequency information, including noise and edges, is contained in the LH, HL, and HH sub-bands. Effective noise estimation depends on the ability to distinguish noise from these high-frequency components. By removing edges in the LH, HL, and HH layers, the noisy components can be identified. Denote $LL_i$, $LH_i$, $HL_i$, and $HH_i$ as frequency layers obtained at decomposition level $i$ in the 2D DWT process. Let $EM$ be the edge detection result applied to the $LL_i$ layer, and $IEM$ be the inverted edge mask. The noise energy map $NEM$ is expressed as</p>
<p>$$IEM = 1 - EM$$
$$NEM = \sqrt{a \cdot LH_i^2 + b \cdot HL_i^2 + c \cdot HH_i^2} \cdot IEM$$</p>
<p>where $a$, $b$, and $c$ are weighting parameters for regulating the influence of $LH_i$, $HL_i$, and $HH_i$ coefficients, respectively.</p>
<p>Noise dominates the edges when an image suffers from severe noise, causing underestimation in the noise influence evaluation. Our proposed approach addresses this problem by recognizing that varying noise energy distributions require different quantiles to identify the most representative energy data for estimation. In real-world scenarios such as video conferencing, image enlargement is commonly applied when a face appears in a frame to make the subject more visually prominent. Since faces before the enlargement are relatively small in the scene, they inherently contain a constrained amount of visual details. Higher decomposition levels can reveal subtle features that might be missed at lower levels, improving the noise estimation performance within small face images. We use the decomposition level of 3 in our method. The edge mask $EM$ is obtained by applying the Sobel operator on the $LL_3$ layer.</p>
<h3 id="feature-hvs-based-sharpness-measure">Feature: HVS-based Sharpness Measure</h3>
<p>Sharpness determines how well-defined the textures and
edges of objects appear in images. Inspired by the previous
task of spatial noise estimation, our proposed image sharpness measure integrates the spatial-domain and transform-domain methods. We employ edge detection on the image’s low-frequency sub-band to effectively capture the edge-related high-frequency components that contain the critical sharpness information. Three levels of DWT edge-related high-frequency sub-bands are then incorpo-
rated into the sharpness calculation. The method, which combines spatial and transform domain information, minimizes the impact of noise as much as possible while emphasizing the importance of edge features when calculating image clarity.</p>

                            </div>
                            
                        </div>
                    </div>
                </div>
            </section>
            

            
            <section class="section">
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column">
                            <h2 class="title is-3 has-text-centered">Results</h2>
                            <div class="content has-text-justified">
                                <p>We utilize two face image datasets in our experiments: our
collected mannequin face images and the MS1MV2 dataset. Our dataset validates quality degradation due to interpolation, while MS1MV2 demonstrates FRIEREN’s effectiveness compared to other IQA methods.</p>

<div class="has-text-centered" style="margin: 1.0rem 0;">
    <div class="method-image-container">
        <img src="https://NTUneillee.github.io/academic-assets/images/TestScene1.webp" alt="**Mannequin dataset**: Three realistic mannequins are used in part of our experiments. To prevent any impact from post-processing on facial details, all images are captured in RAW format using a Sony IMX383-AAQK image sensor." />
        
        <p style="margin-top: 1rem; font-size: 0.95rem; color: #666; line-height: 1.6;"><strong>Mannequin dataset</strong>: Three realistic mannequins are used in part of our experiments. To prevent any impact from post-processing on facial details, all images are captured in RAW format using a Sony IMX383-AAQK image sensor.</p>
        
    </div>
</div>


<div class="has-text-centered" style="margin: 1.0rem 0;">
    <div class="method-image-container">
        <img src="https://NTUneillee.github.io/academic-assets/images/TestScene2.webp" alt="**Mannequin dataset**: In our dataset, gamma correction is applied to generate additional face images, which are enlarged using nearest-neighbor, bilinear, bicubic, and Lanczos interpolation at scales from 2× to 5× (step 0.5). For MS1MV2, 10,000 randomly selected images are upscaled to 2× using all four methods. " />
        
        <p style="margin-top: 1rem; font-size: 0.95rem; color: #666; line-height: 1.6;"><strong>Mannequin dataset</strong>: In our dataset, gamma correction is applied to generate additional face images, which are enlarged using nearest-neighbor, bilinear, bicubic, and Lanczos interpolation at scales from 2× to 5× (step 0.5). For MS1MV2, 10,000 randomly selected images are upscaled to 2× using all four methods.</p>
        
    </div>
</div>

<p>Currently, there are no mean opinion scores (MOS) evaluated by the human subjective judgment for face images enlarged using different interpolations. Since adopting full-reference image quality assessments (FRIQAs) to generate credible pseudo-MOS has proven highly effective, we adopt PSNR as our target metric. Suppose that an original face image is scaled up by a factor of $β$
using each of the four interpolations. To produce the PSNR
value, we first resize the original face image to $1/β$ % of its
size. Next, we enlarge its size by a factor of $β$, restoring
it to the original dimensions. Finally, the PSNR value is
calculated using the original image and the interpolated image.</p>
<p>To target MOS values for each interpolation, we scale the MOSs in proportion to the relative PSNR values. That is, the relationship between the MOS values mirrors the ratio of their corresponding PSNRs. We use CLIB-FIQA, the most advanced face image quality assessment method, to evaluate face images enlarged using Lanczos interpolation. These scores serve as the reference MOS values. After that, all the corresponding target MOS values of each image both in our dataset and in the MS1MV2 dataset are calculated for further regression.</p>
<h3 id="noise-analysis">Noise Analysis</h3>
<p>We hypothesize that the negative influence caused by the nearest-neighbor interpolation can be calculated by the estimations of motion and spatial noise. Temporal noise is calculated using all previous frames in our dataset. For the MS1MV2 dataset, frame modifications are applied for motion noise estimation. The mean estimations of σm and σs for each enlarged face
image in our dataset and the MS1MV2 dataset are listed in Table I and Table II, respectively. The highest results are shown in bold. The results support our theory regarding the side effects of the nearest interpolation on face image quality.</p>
<p><strong>TABLE I:</strong> Average Temporal And Spatial Noise Estimations For Each Type Of Enlarged Image In Our Mannequin Dataset</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center"><strong>Nearest</strong></th>
          <th style="text-align: center"><strong>Bilinear</strong></th>
          <th style="text-align: center"><strong>Bicubic</strong></th>
          <th style="text-align: center"><strong>Lanczos</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Temporal Noise</td>
          <td style="text-align: center"><strong>0.6178</strong></td>
          <td style="text-align: center">0.4880</td>
          <td style="text-align: center">0.5150</td>
          <td style="text-align: center">0.5141</td>
      </tr>
      <tr>
          <td style="text-align: left">Spatial Noise</td>
          <td style="text-align: center"><strong>18.4668</strong></td>
          <td style="text-align: center">14.7780</td>
          <td style="text-align: center">17.6856</td>
          <td style="text-align: center">17.7233</td>
      </tr>
  </tbody>
</table>
<p><strong>TABLE II:</strong> Average Motion And Spatial Noise Estimations For Each Type Of Enlarged Image In The MS1MV2 Dataset</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center"><strong>Nearest</strong></th>
          <th style="text-align: center"><strong>Bilinear</strong></th>
          <th style="text-align: center"><strong>Bicubic</strong></th>
          <th style="text-align: center"><strong>Lanczos</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Motion Noise</td>
          <td style="text-align: center"><strong>4.3453</strong></td>
          <td style="text-align: center">3.2759</td>
          <td style="text-align: center">3.5767</td>
          <td style="text-align: center">3.5807</td>
      </tr>
      <tr>
          <td style="text-align: left">Spatial Noise</td>
          <td style="text-align: center"><strong>25.8520</strong></td>
          <td style="text-align: center">24.1216</td>
          <td style="text-align: center">25.1551</td>
          <td style="text-align: center">25.0978</td>
      </tr>
  </tbody>
</table>
<h3 id="quality-score-prediction">Quality Score Prediction</h3>
<p>Table III and Table IV demonstrate the average PSNR values for each enlarged face image in our dataset and the MS1MV2 dataset, respectively. The highest values are shown in bold. The results also support the subjective human perception of the detail quality rankings in facial images after applying different interpolations: Lanczos, bicubic, bilinear, and nearest-neighbor method (from the best to the worst).</p>
<p><strong>TABLE III:</strong> Average PSNR Results For Each Type Of Enlarged Face Images In Our Mannequin Dataset</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center"><strong>Nearest</strong></th>
          <th style="text-align: center"><strong>Bilinear</strong></th>
          <th style="text-align: center"><strong>Bicubic</strong></th>
          <th style="text-align: center"><strong>Lanczos</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Mean PSNR</td>
          <td style="text-align: center">28.6731</td>
          <td style="text-align: center">29.1761</td>
          <td style="text-align: center">29.3089</td>
          <td style="text-align: center"><strong>29.3140</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>TABLE IV:</strong> Average PSNR Results For Each Type Of Enarged Face Images In The MS1MV2 Dataset</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center"><strong>Nearest</strong></th>
          <th style="text-align: center"><strong>Bilinear</strong></th>
          <th style="text-align: center"><strong>Bicubic</strong></th>
          <th style="text-align: center"><strong>Lanczos</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Mean PSNR</td>
          <td style="text-align: center">29.9370</td>
          <td style="text-align: center">31.7721</td>
          <td style="text-align: center">33.5590</td>
          <td style="text-align: center"><strong>33.5791</strong></td>
      </tr>
  </tbody>
</table>
<p>In addition to the three proposed image features, CLIB-FIQA scores on the original unenlarged images are also used in the regression. Quality prediction performance is assessed using Pearson linear correlation coefficient (PLCC) and Spearman rank order correlation coefficient (SROCC). To
ensure FRIEREN’s robustness, train-validation-test operations are repeated randomly 10 times, with median PLCC and SROCC values reported as final results. The prediction in the MS1MV2 dataset, assessed with FRIEREN and other existing state-of-the-art no-reference IQA methods such as MGVG
(2017), CDV (2024), and CLIB-FIQA (2024), are illustrated in Table
V. The average predicted quality scores for all methods are illustrated in Table VI. The highest values are shown in bold. It is worth noting that CLIB-FIQA is specifically designed for the quality evaluation of face images and trained on the MS1MV2 dataset. FRIEREN’s quality prediction best reflects
the trend of target MOS values across enlarged face images.</p>
<p><strong>TABLE V:</strong> Performance Comparison of Quality Prediction on Testing Face Images in the MS1MV2 Dataset</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center"><strong>Nearest</strong></th>
          <th style="text-align: center"><strong>Bilinear</strong></th>
          <th style="text-align: center"><strong>Bicubic</strong></th>
          <th style="text-align: center"><strong>Lanczos</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">PLCC</td>
          <td style="text-align: center">-0.6411</td>
          <td style="text-align: center">-0.6860</td>
          <td style="text-align: center">0.7816</td>
          <td style="text-align: center"><strong>0.8954</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">SROCC</td>
          <td style="text-align: center">-0.6905</td>
          <td style="text-align: center">-0.6434</td>
          <td style="text-align: center">0.7327</td>
          <td style="text-align: center"><strong>0.8723</strong></td>
      </tr>
  </tbody>
</table>
<p><strong>TABLE VI:</strong> Average Quality Scores of Different IQA Methods in the MS1MV2 Dataset. HVS-preferred Quality Ranking: Lanczos &gt; Bicubic &gt; Bilinear &gt; Nearest-neighbor.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left"></th>
          <th style="text-align: center"><strong>MGVG</strong></th>
          <th style="text-align: center"><strong>CDV</strong></th>
          <th style="text-align: center"><strong>CLIB-FIQA</strong></th>
          <th style="text-align: center"><strong>FRIEREN</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Nearest</td>
          <td style="text-align: center"><strong>56.2486</strong></td>
          <td style="text-align: center"><strong>32.4979</strong></td>
          <td style="text-align: center"><strong>0.7511</strong></td>
          <td style="text-align: center">0.6176</td>
      </tr>
      <tr>
          <td style="text-align: left">Bilinear</td>
          <td style="text-align: center">28.9318</td>
          <td style="text-align: center">16.5068</td>
          <td style="text-align: center">0.7361</td>
          <td style="text-align: center">0.6615</td>
      </tr>
      <tr>
          <td style="text-align: left">Bicubic</td>
          <td style="text-align: center">33.4192</td>
          <td style="text-align: center">19.1522</td>
          <td style="text-align: center">0.7443</td>
          <td style="text-align: center">0.6621</td>
      </tr>
      <tr>
          <td style="text-align: left">Lanczos</td>
          <td style="text-align: center">31.9301</td>
          <td style="text-align: center">18.2963</td>
          <td style="text-align: center">0.7441</td>
          <td style="text-align: center"><strong>0.6629</strong></td>
      </tr>
  </tbody>
</table>

                            </div>
                            
                        </div>
                    </div>
                </div>
            </section>
            <section class="section hero is-light" id="BibTeX">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column">
                        <h2 class="title is-3 has-text-centered">BibTeX</h2>
                        <pre><code>@inproceedings{frieren2025,
  title = {FRIEREN: A Lightweight System for Face Resizing Image Detail Quality Evaluation via Robust Estimation of Image Naturalness},
  author = {Yuan-Kang Lee and Kuan-Lin Chen and Jian-Jiun Ding},
  booktitle = {IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
  year = {2025},
  pages = {to appear},
  url = {https://ntuneillee.github.io/research/friereniqa/}
}
</code></pre>
                    </div>
                </div>
            </div>
        </section>
        

        <footer class="footer">
            <div class="container">
                <div class="columns is-centered">
                    <div class="column is-8">
                        <div class="content has-text-centered">
                            <p>
                                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
                                <br />This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </footer>

        <script>
            
            document.addEventListener('DOMContentLoaded', function() {
                const tables = document.querySelectorAll('.content > table');
                tables.forEach(function(table) {
                    const wrapper = document.createElement('div');
                    wrapper.className = 'table-wrapper';
                    table.parentNode.insertBefore(wrapper, table);
                    wrapper.appendChild(table);
                });
            });
        </script>
    </body>
</html>
